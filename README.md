# Mathematical Proofs in ML/DL for everyone (Especially Korean)
# 머신러닝/딥러닝의 수학적 증명

Proof for algorithms (basic algorithms, ML/DL, etc.)

<h1><b>Contents</b></h1> 
(The ideas are derived from Machine learning with PyTorch and Scikit-Learn by Sebastian Raschka, Yuxi (Hayden) Liu, Vahid Mirjalili)

1. <b>Why does the perceptron converge when the dataset is linearly separable?</b>
2. <b>In the perceptron algorithm, why do we use  y - hat{y}  instead of hat{y} - y ?</b>
3. Upcoming (p.85: Why does the MSE(Mean Squared Error) increase with each epoch when the learning rate is large?)
4. Upcoming (p.86: Why does standardizing the original dataset help with gradient descent learning?)
5. <b>Why is ADALINE called a full-batch gradient descent?</b>
6. <b>The relationship between Logit function and Z = WX+b</b>
7. <b>How to get logistic sigmoid function from logit function?</b>
8. <b>Why do we use 0.5 as threshold of predition in ADALINE algorithm?</b>
9. Upcoming (p.87: Why does MSE not become 0 even if all samples are perfectly classified?)
10. ...

***CAUTION***
1. 대부분의 증명은 영어로 되어 있으며 간혹 답답한 마음에 한글로 작성했습니다.
2. 수학적으로 증명 표기가 정확하지 않을 수 있습니다. (+영어 표현이 이상할 수 있습니다 ㅠ)
3. 오류나 오타 등은 언제든지 연락주세요. (chb3327@yonsei.ac.kr)
